# cli_evaluator.py
#!/usr/bin/env python3
"""
Linux Device Driver Code Evaluator CLI
Usage: python cli_evaluator.py [options]
"""

import argparse
import sys
import os
import json
from typing import List, Dict

# Add src to path
sys.path.append('src')
sys.path.append('.')

from src.evaluator.main_evaluator import DriverModelEvaluator
from tests.test_cases.sample_prompts import SAMPLE_PROMPTS, SAMPLE_CODES

class CLIEvaluator:
    def __init__(self):
        self.evaluator = DriverModelEvaluator()
    
    def evaluate_file(self, filepath: str, prompt_id: str = None, model_name: str = "manual"):
        """Evaluate a single code file"""
        
        if not os.path.exists(filepath):
            print(f"âŒ Error: File {filepath} not found!")
            return
        
        # Read code from file
        try:
            with open(filepath, 'r') as f:
                code = f.read()
        except Exception as e:
            print(f"âŒ Error reading file: {str(e)}")
            return
        
        # Find prompt info or create default
        prompt_info = {'id': prompt_id or 'manual_test', 'difficulty': 'unknown'}
        
        if prompt_id:
            for prompt in SAMPLE_PROMPTS:
                if prompt['id'] == prompt_id:
                    prompt_info = prompt
                    break
        
        # Evaluate
        result = self.evaluator.evaluate_single_code(code, prompt_info, model_name)
        
        # Print summary
        self._print_evaluation_summary(result)
    
    def evaluate_code_string(self, code: str, prompt_id: str = None, model_name: str = "manual"):
        """Evaluate code from string"""
        
        prompt_info = {'id': prompt_id or 'manual_test', 'difficulty': 'unknown'}
        
        if prompt_id:
            for prompt in SAMPLE_PROMPTS:
                if prompt['id'] == prompt_id:
                    prompt_info = prompt
                    break
        
        result = self.evaluator.evaluate_single_code(code, prompt_info, model_name)
        self._print_evaluation_summary(result)
    
    def run_sample_tests(self):
        """Run evaluation on built-in sample codes"""
        
        print("ðŸ§ª Running Sample Test Cases")
        print("=" * 50)
        
        samples = [
            {
                'code': SAMPLE_CODES['good_basic_driver'],
                'prompt_info': {'id': 'sample_good', 'difficulty': 'test'}
            },
            {
                'code': SAMPLE_CODES['bad_basic_driver'],
                'prompt_info': {'id': 'sample_bad', 'difficulty': 'test'}
            }
        ]
        
        report = self.evaluator.batch_evaluate(samples, "sample_test")
        
        print(f"\nðŸ“Š Sample Test Results:")
        print(f"Average Score: {report['overall_performance']['average_score']:.2f}")
        print(f"Score Range: {report['overall_performance']['min_score']:.2f} - {report['overall_performance']['max_score']:.2f}")
    
    def list_prompts(self):
        """List available sample prompts"""
        
        print("\nðŸ“‹ Available Sample Prompts:")
        print("=" * 40)
        
        for i, prompt in enumerate(SAMPLE_PROMPTS, 1):
            print(f"{i}. ID: {prompt['id']}")
            print(f"   Difficulty: {prompt['difficulty']}")
            print(f"   Description: {prompt['prompt'][:100]}...")
            print()
    
    def _print_evaluation_summary(self, result: Dict):
        """Print enhanced evaluation summary with recommendations"""
        
        print("\nðŸ“Š Evaluation Results")
        print("=" * 50)
        
        metadata = result['metadata']
        scores = result['scores']
        
        print(f"Prompt ID: {metadata['prompt_id']}")
        print(f"Model: {metadata['model_name']}")
        print(f"Code Length: {metadata['code_length']} characters")
        print()
        
        # Overall score with grade
        overall = scores['overall_score']
        grade = self._get_grade(overall)
        print(f"ðŸŽ¯ Overall Score: {overall:.2f}/100 ({grade})")
        print()
        
        print("ðŸ“ˆ Category Scores:")
        for category, score in scores.get('category_scores', {}).items():
            bar = self._get_progress_bar(score)
            print(f"  {category.replace('_', ' ').title()}: {score:.1f}/100 {bar}")
        
        print()
        
        # Print key findings
        analysis = result['analysis_details']
        
        if 'compilation' in analysis:
            comp = analysis['compilation']
            status = "âœ… PASS" if comp.get('success') else "âŒ FAIL"
            print(f"ðŸ”§ Compilation: {status}")
            if comp.get('warnings', 0) > 0:
                print(f"  Warnings: {comp['warnings']}")
            if comp.get('errors', 0) > 0:
                print(f"  Errors: {comp['errors']}")
            if comp.get('basic_checks_passed'):
                print(f"  Basic Syntax: âœ… PASS")
        
        if 'security' in analysis:
            sec = analysis['security']
            if sec.get('issues_found', 0) > 0:
                print(f"ðŸ›¡ï¸ Security Issues Found: {sec['issues_found']}")
                for issue in sec.get('issues', [])[:3]:  # Show first 3
                    print(f"  - {issue}")
            else:
                print(f"ðŸ›¡ï¸ Security: âœ… No major issues found")
        
        # Print patterns found
        if 'kernel_patterns' in analysis:
            patterns = analysis['kernel_patterns']
            if patterns.get('patterns_found', 0) > 0:
                print(f"\nðŸ”§ Kernel Patterns Found ({patterns['patterns_found']}):")
                for pattern in patterns.get('patterns', [])[:5]:
                    print(f"  âœ… {pattern}")
        
        # Print recommendations
        recommendations = result.get('recommendations', [])
        if recommendations:
            print(f"\nðŸ’¡ Recommendations for Improvement:")
            for rec in recommendations[:6]:
                print(f"  {rec}")
        
        print()
    
    def _get_grade(self, score: float) -> str:
        """Convert score to letter grade"""
        if score >= 90:
            return "A"
        elif score >= 80:
            return "B"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"
    
    def _get_progress_bar(self, score: float, width: int = 20) -> str:
        """Generate visual progress bar"""
        filled = int((score / 100) * width)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}]"

def main():
    parser = argparse.ArgumentParser(description='Linux Device Driver Code Evaluator')
    parser.add_argument('--file', '-f', help='Evaluate code from file')
    parser.add_argument('--prompt-id', '-p', help='Specify prompt ID for context')
    parser.add_argument('--model', '-m', default='manual', help='Model name for tracking')
    parser.add_argument('--sample-tests', '-s', action='store_true', help='Run built-in sample tests')
    parser.add_argument('--list-prompts', '-l', action='store_true', help='List available prompts')
    parser.add_argument('--code', '-c', help='Evaluate code from command line string')
    
    args = parser.parse_args()
    
    cli = CLIEvaluator()
    
    if args.list_prompts:
        cli.list_prompts()
    elif args.sample_tests:
        cli.run_sample_tests()
    elif args.file:
        cli.evaluate_file(args.file, args.prompt_id, args.model)
    elif args.code:
        cli.evaluate_code_string(args.code, args.prompt_id, args.model)
    else:
        print("ðŸš€ Linux Device Driver Code Evaluator")
        print("=" * 40)
        print("Use --help for usage information")
        print("\nQuick start:")
        print("  python cli_evaluator.py --sample-tests")
        print("  python cli_evaluator.py --list-prompts")
        print("  python cli_evaluator.py --file mydriver.c")
        print("  python cli_evaluator.py --file test_driver.c --prompt-id basic_char_driver")

if __name__ == "__main__":
    main()
