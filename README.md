# Linux Driver Code Evaluation System

## Overview
This project evaluates the quality of Linux device driver code especially code generated by AI models using static analysis and a flexible scoring system.

## Features
- **Static Analysis**: Checks compilation, security, kernel API usage, code quality.
- **Scoring Engine**: Weighted scores by category (compilation, functionality, security, code quality, advanced features).
- **AI Integration**: Mock and OpenAI API support (for code generation).
- **CLI Tools**: Evaluate single files or run batch benchmarks.
- **Sample Drivers**: Test suite for validation.

## Quick Start

1. **Environment Setup**
    ```
    python3 -m venv linux-driver-evaluator-env
    source linux-driver-evaluator-env/bin/activate
    pip install -r requirements.txt
    ```

2. **Evaluate a Driver File**
    ```
    python cli_evaluator.py --file test_samples/excellent_driver.c
    ```

3. **Run All Sample Tests**
    ```
    python cli_evaluator.py --sample-tests
    ```

4. **AI (Mock) Benchmark**
    ```
    python ai_evaluation_pipeline.py --benchmark
    ```

5. **Demo (runs everything)**
    ```
    python demo_system.py
    ```

## Project Structure

- `src/`               — Static analysis and evaluation modules
- `cli_evaluator.py`   — Command-line file/code evaluator
- `ai_evaluation_pipeline.py` — Batch AI code generation + evaluation
- `test_samples/`      — Sample/test drivers for validation
- `results/`           — Evaluation output and reports
- `docs/`              — System documentation

## Scoring Criteria

- **Compilation**: Builds successfully, no major errors (40%)
- **Functionality**: Implements required device driver features (25%)
- **Security**: Checks for safe coding patterns and dangerous calls (20%)
- **Code Quality**: Style, maintainability, documentation (10%)
- **Advanced Features**: Modern kernel patterns, robust cleanup (5%)

_See [AI_USAGE.md](AI_Usage.md) for a disclosure on AI coding assistance._
